{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70070,"status":"ok","timestamp":1757711035625,"user":{"displayName":"Peter Kamushkin","userId":"09163623760211294591"},"user_tz":-180},"id":"KBH7ufaTLtua","outputId":"e45b6e8d-7e14-4d5b-8f80-c94df6f7998d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":12,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3776,"status":"ok","timestamp":1757713189328,"user":{"displayName":"Peter Kamushkin","userId":"09163623760211294591"},"user_tz":-180},"id":"kAWL02U6LxbH","outputId":"242f5aa6-7a4e-40bc-dac2-0d07dbc0dc92"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"]}],"source":["# @title Imports\n","!pip install tensorflow\n","\n","import joblib\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from drive.MyDrive.stocks.pretrained_rl_ppo.scr.check_data import prepare_time_series\n","from drive.MyDrive.stocks.pretrained_rl_ppo.scr.dataset_builder import DatasetBuilderForYourColumns\n","from drive.MyDrive.stocks.pretrained_rl_ppo.scr.ppo_training import train, prepare_datasets, testing_simulation\n","from drive.MyDrive.stocks.pretrained_rl_ppo.scr.backtest_env import BacktestEnv, EnvConfig\n","from drive.MyDrive.stocks.pretrained_rl_ppo.scr.indicators import *"]},{"cell_type":"code","source":["raw_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89},"id":"ecyFAY784PE-","executionInfo":{"status":"ok","timestamp":1757713332537,"user_tz":-180,"elapsed":72,"user":{"displayName":"Peter Kamushkin","userId":"09163623760211294591"}},"outputId":"4bb85911-9b4a-4443-c3b7-91e083d9f470"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [timestamp, Open, High, Low, Close, Volume]\n","Index: []"],"text/html":["\n","  <div id=\"df-8059e5ac-50b5-4dcd-a4b8-0ea5186b926b\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>timestamp</th>\n","      <th>Open</th>\n","      <th>High</th>\n","      <th>Low</th>\n","      <th>Close</th>\n","      <th>Volume</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8059e5ac-50b5-4dcd-a4b8-0ea5186b926b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-8059e5ac-50b5-4dcd-a4b8-0ea5186b926b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-8059e5ac-50b5-4dcd-a4b8-0ea5186b926b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_79e542d9-f59f-4c77-a35d-5668ce55b444\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('raw_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_79e542d9-f59f-4c77-a35d-5668ce55b444 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('raw_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"raw_df","summary":"{\n  \"name\": \"raw_df\",\n  \"rows\": 0,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":885,"status":"ok","timestamp":1757713381335,"user":{"displayName":"Peter Kamushkin","userId":"09163623760211294591"},"user_tz":-180},"id":"4xXSqkjqL6xc","outputId":"45aaacc5-361f-4838-d452-6f42429ee87c"},"outputs":[{"output_type":"stream","name":"stdout","text":["From 2024-01-01 00:00:00+00:00 to 2025-09-10 23:59:00+00:00\n"]}],"source":["# @title Load and check data\n","\n","# PATH_TO_DATA = \"dummy_df.joblib\"  # <-- provide path\n","PATH_TO_DATA = \"/content/drive/MyDrive/stocks/Data/DOGEUSDT/raw_data/DOGEUSDT_1m_2024-01-01_to_2025-09-10.joblib\"\n","START = \"2024-01-01\"\n","END = \"2025-09-10\"\n","TIMESTAMP_COL = \"timestamp\"          # column with timestamps\n","\n","raw_df = joblib.load(PATH_TO_DATA)\n","df = prepare_time_series(raw_df, timestamp_col=TIMESTAMP_COL).loc[START:END]\n","print(f\"From {df.index[0]} to {df.index[-1]}\")"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23156,"status":"ok","timestamp":1757713457205,"user":{"displayName":"Peter Kamushkin","userId":"09163623760211294591"},"user_tz":-180},"id":"uJewKjxHL9vc","outputId":"0cadc1b2-0fdc-4a25-bca6-9afebe8eca54"},"outputs":[{"output_type":"stream","name":"stdout","text":["['timestamp', 'Open', 'High', 'Low', 'Close', 'Volume', 'EMA_35', 'SMA_35', 'ADX_35', 'ATR_35', 'RSI_35', 'MACD_35', 'MACD_signal_35', 'MACD_hist_35', 'STOCH_K_35', 'STOCH_D_35', 'BOLL_MID_35', 'BOLL_UP_35', 'BOLL_DN_35', 'OBV', 'CCI_35', 'WILLR_35', 'MFI_35', 'ROC_35', 'VWAP_35', 'EMA_42', 'SMA_42', 'ADX_42', 'ATR_42', 'RSI_42', 'MACD_42', 'MACD_signal_42', 'MACD_hist_42', 'STOCH_K_42', 'STOCH_D_42', 'BOLL_MID_42', 'BOLL_UP_42', 'BOLL_DN_42', 'CCI_42', 'WILLR_42', 'MFI_42', 'ROC_42', 'VWAP_42', 'EMA_49', 'SMA_49', 'ADX_49', 'ATR_49', 'RSI_49', 'MACD_49', 'MACD_signal_49', 'MACD_hist_49', 'STOCH_K_49', 'STOCH_D_49', 'BOLL_MID_49', 'BOLL_UP_49', 'BOLL_DN_49', 'CCI_49', 'WILLR_49', 'MFI_49', 'ROC_49', 'VWAP_49', 'EMA_56', 'SMA_56', 'ADX_56', 'ATR_56', 'RSI_56', 'MACD_56', 'MACD_signal_56', 'MACD_hist_56', 'STOCH_K_56', 'STOCH_D_56', 'BOLL_MID_56', 'BOLL_UP_56', 'BOLL_DN_56', 'CCI_56', 'WILLR_56', 'MFI_56', 'ROC_56', 'VWAP_56', 'EMA_63', 'SMA_63', 'ADX_63', 'ATR_63', 'RSI_63', 'MACD_63', 'MACD_signal_63', 'MACD_hist_63', 'STOCH_K_63', 'STOCH_D_63', 'BOLL_MID_63', 'BOLL_UP_63', 'BOLL_DN_63', 'CCI_63', 'WILLR_63', 'MFI_63', 'ROC_63', 'VWAP_63', 'EMA_70', 'SMA_70', 'ADX_70', 'ATR_70', 'RSI_70', 'MACD_70', 'MACD_signal_70', 'MACD_hist_70', 'STOCH_K_70', 'STOCH_D_70', 'BOLL_MID_70', 'BOLL_UP_70', 'BOLL_DN_70', 'CCI_70', 'WILLR_70', 'MFI_70', 'ROC_70', 'VWAP_70', 'EMA_77', 'SMA_77', 'ADX_77', 'ATR_77', 'RSI_77', 'MACD_77', 'MACD_signal_77', 'MACD_hist_77', 'STOCH_K_77', 'STOCH_D_77', 'BOLL_MID_77', 'BOLL_UP_77', 'BOLL_DN_77', 'CCI_77', 'WILLR_77', 'MFI_77', 'ROC_77', 'VWAP_77', 'EMA_84', 'SMA_84', 'ADX_84', 'ATR_84', 'RSI_84', 'MACD_84', 'MACD_signal_84', 'MACD_hist_84', 'STOCH_K_84', 'STOCH_D_84', 'BOLL_MID_84', 'BOLL_UP_84', 'BOLL_DN_84', 'CCI_84', 'WILLR_84', 'MFI_84', 'ROC_84', 'VWAP_84', 'EMA_91', 'SMA_91', 'ADX_91', 'ATR_91', 'RSI_91', 'MACD_91', 'MACD_signal_91', 'MACD_hist_91', 'STOCH_K_91', 'STOCH_D_91', 'BOLL_MID_91', 'BOLL_UP_91', 'BOLL_DN_91', 'CCI_91', 'WILLR_91', 'MFI_91', 'ROC_91', 'VWAP_91', 'EMA_98', 'SMA_98', 'ADX_98', 'ATR_98', 'RSI_98', 'MACD_98', 'MACD_signal_98', 'MACD_hist_98', 'STOCH_K_98', 'STOCH_D_98', 'BOLL_MID_98', 'BOLL_UP_98', 'BOLL_DN_98', 'CCI_98', 'WILLR_98', 'MFI_98', 'ROC_98', 'VWAP_98', 'EMA_105', 'SMA_105', 'ADX_105', 'ATR_105', 'RSI_105', 'MACD_105', 'MACD_signal_105', 'MACD_hist_105', 'STOCH_K_105', 'STOCH_D_105', 'BOLL_MID_105', 'BOLL_UP_105', 'BOLL_DN_105', 'CCI_105', 'WILLR_105', 'MFI_105', 'ROC_105', 'VWAP_105', 'EMA_112', 'SMA_112', 'ADX_112', 'ATR_112', 'RSI_112', 'MACD_112', 'MACD_signal_112', 'MACD_hist_112', 'STOCH_K_112', 'STOCH_D_112', 'BOLL_MID_112', 'BOLL_UP_112', 'BOLL_DN_112', 'CCI_112', 'WILLR_112', 'MFI_112', 'ROC_112', 'VWAP_112', 'EMA_119', 'SMA_119', 'ADX_119', 'ATR_119', 'RSI_119', 'MACD_119', 'MACD_signal_119', 'MACD_hist_119', 'STOCH_K_119', 'STOCH_D_119', 'BOLL_MID_119', 'BOLL_UP_119', 'BOLL_DN_119', 'CCI_119', 'WILLR_119', 'MFI_119', 'ROC_119', 'VWAP_119', 'EMA_126', 'SMA_126', 'ADX_126', 'ATR_126', 'RSI_126', 'MACD_126', 'MACD_signal_126', 'MACD_hist_126', 'STOCH_K_126', 'STOCH_D_126', 'BOLL_MID_126', 'BOLL_UP_126', 'BOLL_DN_126', 'CCI_126', 'WILLR_126', 'MFI_126', 'ROC_126', 'VWAP_126', 'EMA_133', 'SMA_133', 'ADX_133', 'ATR_133', 'RSI_133', 'MACD_133', 'MACD_signal_133', 'MACD_hist_133', 'STOCH_K_133', 'STOCH_D_133', 'BOLL_MID_133', 'BOLL_UP_133', 'BOLL_DN_133', 'CCI_133', 'WILLR_133', 'MFI_133', 'ROC_133', 'VWAP_133', 'EMA_140', 'SMA_140', 'ADX_140', 'ATR_140', 'RSI_140', 'MACD_140', 'MACD_signal_140', 'MACD_hist_140', 'STOCH_K_140', 'STOCH_D_140', 'BOLL_MID_140', 'BOLL_UP_140', 'BOLL_DN_140', 'CCI_140', 'WILLR_140', 'MFI_140', 'ROC_140', 'VWAP_140', 'EMA_147', 'SMA_147', 'ADX_147', 'ATR_147', 'RSI_147', 'MACD_147', 'MACD_signal_147', 'MACD_hist_147', 'STOCH_K_147', 'STOCH_D_147', 'BOLL_MID_147', 'BOLL_UP_147', 'BOLL_DN_147', 'CCI_147', 'WILLR_147', 'MFI_147', 'ROC_147', 'VWAP_147', 'EMA_154', 'SMA_154', 'ADX_154', 'ATR_154', 'RSI_154', 'MACD_154', 'MACD_signal_154', 'MACD_hist_154', 'STOCH_K_154', 'STOCH_D_154', 'BOLL_MID_154', 'BOLL_UP_154', 'BOLL_DN_154', 'CCI_154', 'WILLR_154', 'MFI_154', 'ROC_154', 'VWAP_154', 'EMA_161', 'SMA_161', 'ADX_161', 'ATR_161', 'RSI_161', 'MACD_161', 'MACD_signal_161', 'MACD_hist_161', 'STOCH_K_161', 'STOCH_D_161', 'BOLL_MID_161', 'BOLL_UP_161', 'BOLL_DN_161', 'CCI_161', 'WILLR_161', 'MFI_161', 'ROC_161', 'VWAP_161', 'EMA_168', 'SMA_168', 'ADX_168', 'ATR_168', 'RSI_168', 'MACD_168', 'MACD_signal_168', 'MACD_hist_168', 'STOCH_K_168', 'STOCH_D_168', 'BOLL_MID_168', 'BOLL_UP_168', 'BOLL_DN_168', 'CCI_168', 'WILLR_168', 'MFI_168', 'ROC_168', 'VWAP_168', 'EMA_175', 'SMA_175', 'ADX_175', 'ATR_175', 'RSI_175', 'MACD_175', 'MACD_signal_175', 'MACD_hist_175', 'STOCH_K_175', 'STOCH_D_175', 'BOLL_MID_175', 'BOLL_UP_175', 'BOLL_DN_175', 'CCI_175', 'WILLR_175', 'MFI_175', 'ROC_175', 'VWAP_175', 'EMA_182', 'SMA_182', 'ADX_182', 'ATR_182', 'RSI_182', 'MACD_182', 'MACD_signal_182', 'MACD_hist_182', 'STOCH_K_182', 'STOCH_D_182', 'BOLL_MID_182', 'BOLL_UP_182', 'BOLL_DN_182', 'CCI_182', 'WILLR_182', 'MFI_182', 'ROC_182', 'VWAP_182', 'EMA_189', 'SMA_189', 'ADX_189', 'ATR_189', 'RSI_189', 'MACD_189', 'MACD_signal_189', 'MACD_hist_189', 'STOCH_K_189', 'STOCH_D_189', 'BOLL_MID_189', 'BOLL_UP_189', 'BOLL_DN_189', 'CCI_189', 'WILLR_189', 'MFI_189', 'ROC_189', 'VWAP_189', 'EMA_196', 'SMA_196', 'ADX_196', 'ATR_196', 'RSI_196', 'MACD_196', 'MACD_signal_196', 'MACD_hist_196', 'STOCH_K_196', 'STOCH_D_196', 'BOLL_MID_196', 'BOLL_UP_196', 'BOLL_DN_196', 'CCI_196', 'WILLR_196', 'MFI_196', 'ROC_196', 'VWAP_196', 'EMA_203', 'SMA_203', 'ADX_203', 'ATR_203', 'RSI_203', 'MACD_203', 'MACD_signal_203', 'MACD_hist_203', 'STOCH_K_203', 'STOCH_D_203', 'BOLL_MID_203', 'BOLL_UP_203', 'BOLL_DN_203', 'CCI_203', 'WILLR_203', 'MFI_203', 'ROC_203', 'VWAP_203', 'EMA_210', 'SMA_210', 'ADX_210', 'ATR_210', 'RSI_210', 'MACD_210', 'MACD_signal_210', 'MACD_hist_210', 'STOCH_K_210', 'STOCH_D_210', 'BOLL_MID_210', 'BOLL_UP_210', 'BOLL_DN_210', 'CCI_210', 'WILLR_210', 'MFI_210', 'ROC_210', 'VWAP_210', 'EMA_217', 'SMA_217', 'ADX_217', 'ATR_217', 'RSI_217', 'MACD_217', 'MACD_signal_217', 'MACD_hist_217', 'STOCH_K_217', 'STOCH_D_217', 'BOLL_MID_217', 'BOLL_UP_217', 'BOLL_DN_217', 'CCI_217', 'WILLR_217', 'MFI_217', 'ROC_217', 'VWAP_217', 'EMA_224', 'SMA_224', 'ADX_224', 'ATR_224', 'RSI_224', 'MACD_224', 'MACD_signal_224', 'MACD_hist_224', 'STOCH_K_224', 'STOCH_D_224', 'BOLL_MID_224', 'BOLL_UP_224', 'BOLL_DN_224', 'CCI_224', 'WILLR_224', 'MFI_224', 'ROC_224', 'VWAP_224', 'EMA_231', 'SMA_231', 'ADX_231', 'ATR_231', 'RSI_231', 'MACD_231', 'MACD_signal_231', 'MACD_hist_231', 'STOCH_K_231', 'STOCH_D_231', 'BOLL_MID_231', 'BOLL_UP_231', 'BOLL_DN_231', 'CCI_231', 'WILLR_231', 'MFI_231', 'ROC_231', 'VWAP_231', 'EMA_238', 'SMA_238', 'ADX_238', 'ATR_238', 'RSI_238', 'MACD_238', 'MACD_signal_238', 'MACD_hist_238', 'STOCH_K_238', 'STOCH_D_238', 'BOLL_MID_238', 'BOLL_UP_238', 'BOLL_DN_238', 'CCI_238', 'WILLR_238', 'MFI_238', 'ROC_238', 'VWAP_238', 'EMA_245', 'SMA_245', 'ADX_245', 'ATR_245', 'RSI_245', 'MACD_245', 'MACD_signal_245', 'MACD_hist_245', 'STOCH_K_245', 'STOCH_D_245', 'BOLL_MID_245', 'BOLL_UP_245', 'BOLL_DN_245', 'CCI_245', 'WILLR_245', 'MFI_245', 'ROC_245', 'VWAP_245']\n"]}],"source":["# @title Add indicators\n","\n","# Базовые массивы (создаём один раз)\n","close  = df['Close'].to_numpy(np.float64)\n","high   = df['High'].to_numpy(np.float64)\n","low    = df['Low'].to_numpy(np.float64)\n","volume = df['Volume'].to_numpy(np.float64)\n","\n","# Словарь для новых колонок\n","new_cols = {}\n","obv_arr = obv_numba(close, volume)\n","\n","for i in range(35, 246, 7):\n","    ema_i = ema_numba(close, i)\n","    sma_i = sma_numba(close, i)\n","    adx_i = adx_numba(high, low, close, i)\n","    atr_i = atr_numba(high, low, close, i)\n","    rsi_i = rsi_numba(close, i)\n","\n","    fast = i\n","    slow = i * 2\n","    sig  = i // 2  # вместо int(i/2)\n","\n","    macd, macd_signal, macd_hist = macd_numba(close, fast, slow, sig)\n","    stoch_k, stoch_d = stoch_numba(high, low, close, i, i // 2)\n","    boll_mid, boll_up, boll_dn = bollinger_numba(close, i, 2.0)\n","    cci_i  = cci_numba(high, low, close, i)\n","    willr_i= williams_r_numba(high, low, close, i)\n","    mfi_i  = mfi_numba(high, low, close, volume, i)\n","    roc_i  = roc_numba(close, i)\n","    vwap_i = vwap_numba(high, low, close, volume, i)  # если это rolling VWAP на окне i\n","\n","    new_cols.update({\n","        f\"EMA_{i}\":          ema_i,\n","        f\"SMA_{i}\":          sma_i,\n","        f\"ADX_{i}\":          adx_i,\n","        f\"ATR_{i}\":          atr_i,\n","        f\"RSI_{i}\":          rsi_i,\n","        f\"MACD_{i}\":         macd,\n","        f\"MACD_signal_{i}\":  macd_signal,\n","        f\"MACD_hist_{i}\":    macd_hist,\n","        f\"STOCH_K_{i}\":      stoch_k,\n","        f\"STOCH_D_{i}\":      stoch_d,\n","        f\"BOLL_MID_{i}\":     boll_mid,\n","        f\"BOLL_UP_{i}\":      boll_up,\n","        f\"BOLL_DN_{i}\":      boll_dn,\n","        f\"OBV\":              obv_arr,\n","        f\"CCI_{i}\":          cci_i,\n","        f\"WILLR_{i}\":        willr_i,\n","        f\"MFI_{i}\":          mfi_i,\n","        f\"ROC_{i}\":          roc_i,\n","        f\"VWAP_{i}\":         vwap_i,\n","    })\n","\n","# Добавляем все столбцы разом — без фрагментации\n","df = pd.concat([df, pd.DataFrame(new_cols, index=df.index)], axis=1)\n","\n","print(df.columns.to_list())"]},{"cell_type":"code","execution_count":19,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":910,"status":"ok","timestamp":1757713458120,"user":{"displayName":"Peter Kamushkin","userId":"09163623760211294591"},"user_tz":-180},"id":"5B1St2NY0QOB","outputId":"83783871-813f-4565-9b73-65d8ea5c8c4f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[('2024-01-01', 1776), ('2024-01-10', 2541), ('2024-01-20', 2310), ('2024-01-26', 1222), ('2024-01-27', 1709), ('2024-01-29', 1642), ('2024-02-07', 1977), ('2024-02-08', 2010), ('2024-02-10', 1059), ('2024-02-12', 1321), ('2024-02-14', 2058), ('2024-02-18', 2343), ('2024-02-24', 2756), ('2024-02-26', 4784), ('2024-03-01', 2791), ('2024-03-03', 3124), ('2024-03-08', 1050), ('2024-03-09', 1259), ('2024-03-20', 2590), ('2024-03-23', 3404), ('2024-03-25', 1506), ('2024-03-27', 2636), ('2024-04-06', 4211), ('2024-04-20', 1491), ('2024-05-02', 1221), ('2024-05-03', 4396), ('2024-05-09', 1141), ('2024-05-13', 1183), ('2024-05-15', 1804), ('2024-05-17', 1377), ('2024-05-20', 2698), ('2024-05-25', 2070), ('2024-06-04', 2219), ('2024-06-25', 2011), ('2024-06-27', 1328), ('2024-06-30', 1144), ('2024-07-06', 1676), ('2024-07-12', 2389), ('2024-07-14', 2512), ('2024-07-16', 1866), ('2024-07-19', 3126), ('2024-07-21', 1846), ('2024-07-26', 2444), ('2024-08-08', 2154), ('2024-08-10', 1376), ('2024-08-22', 2917), ('2024-09-02', 1063), ('2024-09-08', 3163), ('2024-09-13', 1868), ('2024-09-18', 1705), ('2024-09-21', 1659), ('2024-09-26', 3057), ('2024-09-28', 1341), ('2024-10-06', 1641), ('2024-10-11', 3430), ('2024-10-14', 1922), ('2024-10-15', 1909), ('2024-10-17', 3914), ('2024-10-27', 4567), ('2024-11-04', 3402), ('2024-11-08', 6379), ('2024-11-13', 1054), ('2024-11-19', 1299), ('2024-11-22', 1758), ('2024-11-27', 1053), ('2024-11-29', 1858), ('2024-12-05', 1060), ('2024-12-07', 1622), ('2024-12-24', 1514), ('2024-12-28', 1672), ('2025-01-01', 5448), ('2025-01-06', 1258), ('2025-01-13', 2314), ('2025-01-15', 1573), ('2025-01-16', 2280), ('2025-01-30', 1326), ('2025-02-08', 1225), ('2025-02-10', 1318), ('2025-02-13', 2503), ('2025-02-28', 1129), ('2025-03-01', 1181), ('2025-03-05', 1260), ('2025-03-14', 2919), ('2025-03-23', 5084), ('2025-04-04', 1411), ('2025-04-09', 1392), ('2025-04-11', 3629), ('2025-04-18', 1442), ('2025-04-22', 2900), ('2025-04-24', 2615), ('2025-05-01', 2223), ('2025-05-06', 1045), ('2025-05-07', 5438), ('2025-05-11', 1272), ('2025-05-13', 1038), ('2025-05-20', 1319), ('2025-05-21', 2420), ('2025-06-09', 1515), ('2025-06-10', 1601), ('2025-06-23', 2754), ('2025-06-28', 2445), ('2025-07-02', 2353), ('2025-07-06', 1491), ('2025-07-08', 5527), ('2025-07-16', 2389), ('2025-07-17', 1452), ('2025-07-18', 1480), ('2025-07-19', 2842), ('2025-08-04', 1876), ('2025-08-06', 5322), ('2025-08-12', 2478), ('2025-08-17', 1042), ('2025-08-22', 2420), ('2025-09-03', 1542), ('2025-09-06', 4405), ('2025-09-10', 1103)]\n"]}],"source":["# @title Разметка тренда\n","\n","def get_intervals(df: pd.DataFrame, col: str, treshold: int = 1000) -> list[tuple[str, int]]:\n","    \"\"\"\n","    Находит интервалы от сигнала 1 до сигнала -1.\n","\n","    :param df: DataFrame с datetime-индексом и колонкой сигналов {-1, 0, 1}\n","    :param col: имя колонки с сигналами\n","    :return: список кортежей (начало_как_str 'YYYY-MM-DD', длина_в_строках)\n","    \"\"\"\n","    intervals = []\n","    start_idx = None\n","    start_pos = None  # позиция для подсчета длины\n","\n","    for pos, (idx, val) in enumerate(df[col].items()):\n","        if val == 1 and start_idx is None:\n","            start_idx = idx\n","            start_pos = pos\n","        elif val == -1 and start_idx is not None:\n","            length = pos - start_pos + 1\n","            if length > treshold:\n","                intervals.append((start_idx.strftime(\"%Y-%m-%d\"), length))\n","            start_idx = None\n","            start_pos = None\n","\n","    return intervals\n","\n","df['EMA_2500'] = ema_numba(close, 2500)\n","\n","buy_sig  = (df['Close'] > df['EMA_2500'])\n","sell_sig = (df['Close'] < df['EMA_2500'])\n","\n","df['Signal_Rule'] = np.where(buy_sig, 1, np.where(sell_sig, -1, 0)).astype(np.int8)\n","\n","INDEXES = get_intervals(df, 'Signal_Rule')\n","print(INDEXES)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Tm9WRIlUcuI5"},"outputs":[],"source":["# @title Объяснение параметров и метрик\n","\"\"\"\n","PPO_UPDATES — количество крупных циклов обучения (policy updates). Каждый цикл включает:\n","- Сбор PPO_ROLLOUT шагов из PPO_N_ENV параллельных окружений.\n","- Разбиение собранных данных на мини-батчи PPO_BATCH_SIZE.\n","- PPO_EPOCHS проходов оптимизации по этим батчам.\n","Таким образом, общее число шагов взаимодействия со средой за весь запуск равно PPO_UPDATES × PPO_N_ENV × PPO_ROLLOUT.\n","На что влияет:\n","- Общее время и объём обучения. Большее значение → больше данных и gradient-обновлений → дольше тренировка, выше шанс достичь лучшей политики.\n","- Риск переобучения.При фиксированных данных каждая лишняя итерация может привести к “запоминанию” конкретных траекторий и ухудшению обобщающей способности.\n","- Использование вычислительных ресурсов. PPO_UPDATES напрямую задаёт количество итераций оптимизации, что определяет нагрузку на GPU/CPU.\n","Как подбирать:\n","- Базовая точка: 50 циклов — типичное стартовое значение для средних по сложности задач (≈1 млн шагов при 40 × 512 × 50).\n","- Если агент недоучивается (наградная кривая всё ещё растёт) — увеличьте параметр (например, до 100–200), чтобы дать модели больше времени на освоение среды.\n","- Если награда стабилизируется или падает до завершения обучения — уменьшайте PPO_UPDATES или используйте раннюю остановку по метрике валидации.\n","- Ориентир по шагам среды: выберите целевой объём взаимодействий (например, 2 млн шагов) и рассчитайте PPO_UPDATES = целевые_шаги / (PPO_N_ENV × PPO_ROLLOUT).\n","- Грамотная настройка PPO_UPDATES помогает балансировать между временем обучения, объёмом данных и качеством итоговой политики.\n","\n","\n","PPO_N_ENV - задаёт число окружений (environment instances), запускаемых одновременно. Каждый из них параллельно генерирует свои траектории взаимодействия с миром.\n","Зачем нужны несколько сред:\n","- Быстрая генерация данных. Одновременно работающие окружения собирают шаги опыта параллельно, что ускоряет обучение: за один и тот же реальный промежуток времени вы получаете PPO_N_ENV раз больше данных.\n","- Снижение корреляции. Последовательные шаги в одной среде сильно коррелированы. Запуск множества независимых эпизодов уменьшает зависимость между образцами, что делает оценку градиента более надёжной.\n","- Стабильное обучение. При большем разнообразии исходных состояний и траекторий политика быстрее покрывает пространство состояний, что помогает избежать локальных минимумов.\n","Как подбирать:\n","- Ограничения железа. Увеличение PPO_N_ENV потребляет больше CPU/GPU и памяти. Выбирайте значение, которое эффективно загружает доступные ресурсы (типично 8–64).\n","- Размер батча. Общее число шагов на одно обновление равно PPO_N_ENV × PPO_ROLLOUT. Если вам нужен крупный батч (например, для стабильности), проще увеличить PPO_N_ENV, чем сильно удлинять PPO_ROLLOUT.\n","- Время взаимодействия. Если среда медленная или взаимодействие дорогое, увеличивайте PPO_N_ENV, чтобы по максимуму распараллелить сбор данных.\n","- Когда уменьшать. Если компьютер не справляется (CPU загружен на 100%, задержки растут), снизьте число сред или используйте асинхронную схему (A3C, RLLib).\n","Правильное значение PPO_N_ENV — компромисс между скоростью обучения и доступными ресурсами; начинайте с числа, оптимально загружающего ваш сервер, и корректируйте в зависимости от стабильности и производительности обучения.\n","\n","\n","PPO_ROLLOUT - задаёт длину собранной траектории из каждого окружения перед одним обновлением политики.\n","Если PPO_N_ENV = 40 и PPO_ROLLOUT = 512, то за цикл обучения собирается 40 × 512 = 20 480 шагов взаимодействия.\n","На что влияет:\n","- Размер батча и скорость обновления. Длинный rollout увеличивает объём данных на обновление и реже вызывает оптимизацию, что делает каждое обновление более «массовым», но редким.\n","- Корреляция данных. Чем длиннее траектория, тем более последовательные (коррелированные) шаги попадают в выборку. Это может замедлить обучение,\n","особенно в задачах с высокой автокорреляцией.\n","- Обновление значений и преимущества. Rollout определяет глубину вычисления возвратов/GAE. Слишком короткий сегмент даёт «обрезанные» оценки,\n","а слишком длинный может приводить к устареванию данных, если политика за время сбора уже изменилась.\n","- Потребление памяти. Хранение длинных траекторий требует больше RAM/VRAM, что может ограничить максимальное значение на слабом оборудовании.\n","Как подбирать:\n","- Стартовое значение: 128–512 шагов — типичный диапазон для большинства задач в игре или управлении роботами.\n","- Баланс с PPO_N_ENV: поддерживайте общий размер батча (PPO_N_ENV × PPO_ROLLOUT) в разумных пределах (например, 8 000–30 000 шагов). Если увеличиваете число сред, rollout можно уменьшить.\n","- Если обучение шумное и нестабильное: попробуйте удлинить rollout, чтобы каждое обновление опиралось на большее количество данных.\n","- Если среда очень медленная: укорачивайте rollout и/или увеличивайте PPO_N_ENV, чтобы чаще обновлять политику и быстрее получать обратную связь.\n","- Анализ качества: отслеживайте кривые награды и скорость сходимости; оптимальное значение rollout — то, при котором прирост награды стабилен, а время до обновления не слишком велико.\n","Итог: PPO_ROLLOUT задаёт, сколько последовательных шагов собирает каждая среда до обновления. Он напрямую влияет на размер батча, корреляцию данных и частоту обновлений, поэтому выбирать его нужно в связке с количеством параллельных окружений и возможностями оборудования.\n","\n","\n","PPO_CLIP_RATIO — центральный гиперпараметр в алгоритме Proximal Policy Optimization. Его называют «шириной зажима» и обычно обозначают ε.\n","В PPO вычисляется отношение вероятностей действий новой политики и старой. Чтобы не позволять обновлённой политике слишком далеко «отклоняться» от старой, каждая величина\n","r(θ) проходит через функцию clip(r, 1-ε, 1+ε) и затем берётся минимум.\n","На что влияет:\n","- Стабильность обучения. Чем меньше PPO_CLIP_RATIO, тем более «осторожные» обновления: политика меняется медленно, но обучение устойчивее.\n","- Скорость и амплитуда обновлений.Больший параметр позволяет быстрее приспосабливаться к изменениям среды — но есть риск «разлета» градиентов и деградации.\n","- Баланс исследования/эксплуатации. При небольшом ε алгоритм дольше сохраняет свойства старой политики, что снижает риск забывания, но может замедлить улучшение.\n","Как подбирать:\n","- Стартовое значение: 0.1–0.3 (типичный базовый диапазон из статьи PPO).\n","- Если обучение нестабильно или награда сильно колеблется, уменьшите ε (например, 0.1 или 0.05).\n","- Если модель «залипает» и улучшается медленно, попробуйте увеличить (0.3 и выше), но контролируйте, чтобы потери и дивергенция не расходились.\n","- Иногда применяют адаптивную схему: отслеживают фактический KL-дивергенцию между старой и новой политиками и динамически корректируют PPO_CLIP_RATIO, но это усложняет реализацию.\n","Итого: PPO_CLIP_RATIO — это безопасный «коридор», внутри которого политика может двигаться за один шаг обновления. Правильная настройка этого параметра критична для того, чтобы обучение было и стабильным, и достаточно быстрым.\n","\n","\n","PPO_MAX_GRAD_NORM - задаёт верхнее ограничение на норму (длину) вектора градиента при обновлении параметров сети.\n","После вычисления градиентов они при необходимости «обрезаются» так, чтобы их норма не превышала это значение.\n","На что влияет:\n","- Стабильность обучения. Ограничение градиентов предотвращает резкие скачки параметров и защищает от взрывных градиентов, особенно при высокой скорости обучения.\n","- Сходимость. При корректном «клиппинге» градиента модель движется к минимуму более плавно, что способствует более надёжной сходимости.\n","- Скорость обучения. Слишком низкий порог может замедлить адаптацию, поскольку сильные градиенты будут «срезаться», а слишком высокий — не окажет должной стабилизации.\n","Как подбирать:\n","- Стартовое значение: 0.5–1.0 — типичный диапазон для большинства задач с PPO.\n","- Если наблюдаются взрывы градиента (loss резко скачет, параметры становятся nan), уменьшите значение.\n","- Если обучение идёт слишком медленно и градиенты редко достигают порога, можно слегка увеличить PPO_MAX_GRAD_NORM.\n","- Комбинируйте с learning rate: при повышении PPO_ACTOR_LR или PPO_CRITIC_LR может потребоваться более жёсткое ограничение градиента.\n","Итого: PPO_MAX_GRAD_NORM — «страховочный трос» для градиентов, удерживающий шаги оптимизации в разумных пределах.\n","Его грамотная настройка помогает избежать нестабильности и сохранить баланс между скоростью и надёжностью обучения.\n","\n","\n","PPO_TARGET_KL — целевое значение KL-дивергенции между старой и новой политиками. После каждого обновления алгоритм измеряет фактическую KL-дивергенцию и сравнивает её с целевым порогом.\n","На что влияет:\n","- Контроль шага обновления. Если реальная дивергенция превышает PPO_TARGET_KL, дальнейшие итерации оптимизации останавливаются или замедляются — это предотвращает слишком резкие изменения политики.\n","- Стабильность обучения. Поддержание дивергенции вблизи целевого значения способствует более плавной и предсказуемой траектории обучения.\n","- Компромисс между скоростью и надёжностью. Более низкий порог делает обучение осторожным, но может замедлить прогресс; более высокий — ускоряет адаптацию, но повышает риск нестабильности.\n","Как подбирать:\n","- Стартовое значение: 0.01 — распространённый базовый выбор.\n","- Если наблюдается «разлёт» политики (потери нестабильны, награда скачет), попробуйте уменьшить PPO_TARGET_KL, чтобы жёстче контролировать изменения.\n","- Если обучение идёт слишком медленно и KL практически не достигает порога, значение можно поднять (например, до 0.02–0.05), позволяя большей свободе обновлений.\n","- Адаптивные схемы: иногда вводят постепенное снижение порога по ходу обучения, чтобы сначала быстро исследовать, а потом стабилизироваться.\n","Итого: PPO_TARGET_KL служит «стоп-сигналом» для слишком больших отклонений новой политики от старой.\n","Правильно подобранный порог помогает сочетать быстрое обучение с контролируемой стабильностью, что особенно важно при длительных или чувствительных к разрывам задачах.\n","\n","\n","PPO_GAMMA (discount factor) — коэффициент дисконтирования будущих вознаграждений.\n","Значение ближе к 1 делает агент более «дальнозорким», учитывая долгосрочные последствия действий.\n","Более низкое значение заставляет агента концентрироваться на ближайшем вознаграждении.\n","Типичные значения: 0.99, 0.95 — выбираются в зависимости от глубины временного горизонта задачи.\n","\n","\n","PPO_LAM (lambda) — параметр обобщённой оценки преимущества (GAE), регулирующий баланс между bias и variance.\n","Значение 1.0 приводит к меньшему смещению, но более высокому разбросу.\n","Значение 0.0 повышает смещение, но уменьшает разброс, что делает обучение более «шумным».\n","Часто используется 0.95–0.97, но оптимальное значение зависит от особенностей среды и политики.\n","Практика подбора:\n","- Начать с общепринятых значений (gamma=0.99, lam=0.95).\n","- Выполнить поиск по сетке (grid search) или использованию систематических экспериментов, меняя по одному параметру.\n","- Анализировать метрики обучения (скорость сходимости, стабильность, итоговое качество политики) и при необходимости корректировать параметры.\n","\n","\n","МЕТРИКИ:\n","\n","policy_loss = -0.01140\n","Отражает, насколько сильно обновление политики снижает целевую функцию. Небольшое отрицательное значение говорит о том,\n","что текущие обновления улучшают политику, но шаги достаточно осторожные. Если абсолютное значение постоянно растёт (становится более отрицательным),\n","модель активно адаптируется; если стремится к нулю — политика стабилизируется.\n","\n","value_loss = 0.72330\n","Показывает ошибку критика при оценке ожидаемого вознаграждения. Чем меньше, тем точнее предсказания.\n","Значение выше ~0.7 может означать, что критик ещё не до конца обучен или задача сложная.\n","Если loss со временем снижается — критик становится точнее; рост может указывать на переобучение политики или слишком высокую скорость обучения критика.\n","\n","entropy = 0.43223\n","Измеряет “случайность” действий (степень исследования). 0.43 — умеренная энтропия: политика сохраняет разнообразие, но уже не совершенно случайна.\n","По мере обучения энтропия обычно убывает: слишком быстрое падение — риск преждевременной детерминированности, слишком медленное — модель долго остаётся хаотичной.\n","\n","approx_kl = 0.05351\n","Приблизительная KL-дивергенция между старой и новой политиками. Она показывает, насколько сильно изменяется политика после обновления.\n","Значение 0.0535 в пять раз превышает ваш целевой PPO_TARGET_KL = 0.01, что может сигнализировать о слишком резких шагах.\n","Если approx_kl стабильно выше порога, стоит уменьшить PPO_CLIP_RATIO или скорость обучения.\n","\n","teacher_kl = 0.0457\n","Среднее значение KL-дивергенции между текущей политикой и teacher‑моделью. В коде собранные на каждой итерации t_kl складываются в список teacher_kls, а затем усредняются.\n","Значение ~0.046 говорит, что политика умеренно отклоняется от teacher: выше нуля (политика учится отличаться), но далеко от заданного лимита 0.1.\n","\n","clip_fraction = 0.0124\n","Доля шагов, где отношение вероятностей действий вышло за пределы 1 ± PPO_CLIP_RATIO и была применена обрезка.\n","В коде эти события накапливаются в clipfracs, после чего берётся их среднее. Низкое значение (~1.2 %) означает,\n","что обновления оставались в пределах допустимого коридора, и политика изменялась достаточно плавно.\n","\n","avg_return = -0.1172\n","Среднее возвращённое вознаграждение по собранной траектории за обновление.\n","В коде вычисляется как среднее значение traj.returns и записывается в метрики.\n","Отрицательная величина показывает, что агент в среднем пока приносит убыток — требуется дальнейшее обучение или настройка гиперпараметров.\n","\n","Что означают изменения во времени\n","Падение policy_loss и value_loss — признак прогресса, но отслеживайте, чтобы value_loss не рос после изначального падения.\n","Снижение энтропии — нормальный процесс, но важно, чтобы она не опускалась слишком рано; в противном случае увеличьте PPO_C2.\n","Пики approx_kl выше целевого порога — сигнал, что обновления могут быть слишком агрессивными;\n","при регулярном превышении значения следует уменьшить PPO_ACTOR_LR, PPO_CLIP_RATIO или число эпох PPO_EPOCHS.\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9dj0uc8MEyB","cellView":"form"},"outputs":[],"source":["# @title Обучение на весах SL модели\n","SL_MODEL_CONFIG = \"/content/drive/MyDrive/stocks/pretrained_rl_ppo/sl_weights/exp3/config.joblib\"\n","PPO_SAVE_PATH = '/content/drive/MyDrive/stocks/pretrained_rl_ppo/ppo_weights/exp3'\n","CONFIG = joblib.load(SL_MODEL_CONFIG)\n","\n","SPLITS = (0.80, 0.15, 0.05)\n","BEST_MODEL_PATH = \"/content/drive/MyDrive/stocks/pretrained_rl_ppo/sl_weights/exp3/best_full_model_ep:48_f1:68.3.weights.h5\"\n","BEST_BACKBONE_PATH = \"/content/drive/MyDrive/stocks/pretrained_rl_ppo/sl_weights/exp3/best_backbone__ep:37_f1:68.1.weights.h5\"\n","BEST_CRITIC_PATH = \"\" # нет потому что в SL мы не учим критика, вместо этого берем backbone\n","FEATURES = CONFIG['features']\n","ACCOUNT_F = CONFIG['state']\n","\n","# @title PPO training parameters\n","PPO_SEQ_LEN = CONFIG['seq_len'] # ширина окна (такая же как при обучении SL)\n","PPO_FEATURES_DIM = len(FEATURES) + len(ACCOUNT_F)\n","PPO_NUM_ACTIONS = CONFIG['num_actions'] # кол-во возможных действий (такое же как при обучении SL)\n","PPO_UNITS_PER_LAYER = CONFIG['units_per_layer'] #  кол-во нейронов (такое же как при обучении SL)\n","PPO_DROPOUT = CONFIG['dropout']\n","\n","# PPO params\n","PPO_UPDATES = 20         # число обновлений политики за цикл обучения (эпохи)\n","PPO_N_ENV = 30          # количество параллельных окружений для сбора данных\n","PPO_N_VAL = 5           # кол-во валидаций на один update\n","PPO_ROLLOUT = 512       # длина траектории (количество шагов), собираемой перед обновлением\n","PPO_ACTOR_LR = 3e-3     # скорость обучения актёра\n","PPO_CRITIC_LR = 1e-3    # скорость обучения критика\n","PPO_CLIP_RATIO = 0.5    # центральный гиперпараметр в алгоритме Proximal Policy Optimization. Его называют «шириной зажима»\n","PPO_C1 = 0.5            # коэффициент перед вкладом ошибки критика в функции потерь\n","PPO_C2 = 0.03           # коэффициент перед бонусом энтропии в функции потерь (чем больше, тем больше эксплорейшн)\n","PPO_GAMMA = 0.99        # (discount factor) — коэффициент дисконтирования будущих вознаграждений.\n","PPO_LAM = 0.95          # параметр обобщённой оценки преимущества (GAE), регулирующий баланс между bias и variance.\n","PPO_EPOCHS = 20         # количество проходов по батчу при обучении\n","PPO_BATCH_SIZE = 256    # размер мини-батча для SGD\n","PPO_TEACHER_KL = 0.1    # начальный вес KL‑регуляризации с teacher‑моделью\n","PPO_KL_DECAY = 0.95      # коэффициент экспоненциального затухания веса KL\n","PPO_TARGET_KL = 0.01    # целевое значение KL‑дивергенции для ранней остановки\n","PPO_MAX_GRAD_NORM = 0.5 # ограничение нормы градиента для стабилизации обучения\n","PPO_VAL_INTERVAL = 2    # периодичность валидации (через сколько циклов обучения)\n","PPO_DEBUG = True        # режим вывода отладочной информации\n","\n","# INDEXES = [(\"2024-05-01\", 1000), (\"2024-07-01\", 500)] # если хочешь учить на конкретных отрезках данных\n","\n","# Подготавливаем данные и вычисляем статистики нормализации\n","ppo_train_df, ppo_val_df, ppo_test_df, _feat_stats = prepare_datasets(df,\n","                                                                      FEATURES,\n","                                                                      splits=SPLITS,\n","                                                                      norm_kind='minmax')\n","\n","\n","TRAIN_CFG = EnvConfig(\n","    mode=1,                             # лонг = 1, шорт = -1\n","    fee=0.0002,                         # комиссия\n","    spread=0.0   ,                      # задать, если цена- это midl_price (иначе 0)\n","    leverage=1.0,                       # плечо\n","    max_steps=PPO_ROLLOUT+PPO_SEQ_LEN,  # ограничение по длине эпизода\n","    reward_scale=80.0,                 # вес пнл награды (у пнл обычно маленькое значение 0,001)\n","    use_log_reward=False,               # применение вместо относительного пнл, логарифма пнл\n","    valid_time=60,                       # задержка после которой начинат начисляться штраф\n","    time_penalty=0.0001,                # штраф за бездействие (np.sqrt(max(0, wait_steps -5) * time_penalty))\n","    hold_penalty=0.0001,                # штраф за удержание (np.sqrt(max(0, hold_steps -5) * hold_penalty))\n",")\n","\n","VAL_CFG = EnvConfig(\n","    mode=1,\n","    fee=0.0002,\n","    spread=0.0,\n","    leverage=1.0,\n","    max_steps=1000, #размер валидацилнной выборки\n","    reward_scale=80.0,\n","    use_log_reward=False,\n","    valid_time=60,\n","    time_penalty=0.0001,\n","    hold_penalty=0.0001,\n",")\n","\n","actor, critic, train_log, val_log = train(\n","    ppo_train_df,\n","    ppo_val_df,\n","    TRAIN_CFG,\n","    VAL_CFG,\n","    PPO_FEATURES_DIM,\n","    FEATURES,\n","    seq_len = PPO_SEQ_LEN,\n","    teacher_weights = BEST_MODEL_PATH,\n","    critic_weights = BEST_CRITIC_PATH,\n","    backbone_weights = BEST_BACKBONE_PATH,\n","    save_path = PPO_SAVE_PATH,\n","    num_actions = PPO_NUM_ACTIONS,\n","    units_per_layer = PPO_UNITS_PER_LAYER,\n","    dropout = PPO_DROPOUT,\n","    updates = PPO_UPDATES,\n","    n_env = PPO_N_ENV,\n","    n_validations = PPO_N_VAL,\n","    rollout = PPO_ROLLOUT,\n","    actor_lr = PPO_ACTOR_LR,\n","    critic_lr = PPO_CRITIC_LR,\n","    clip_ratio = PPO_CLIP_RATIO,\n","    c1 = PPO_C1,\n","    c2 = PPO_C2,\n","    gamma = PPO_GAMMA,\n","    lam = PPO_LAM,\n","    epochs = PPO_EPOCHS,\n","    batch_size = PPO_BATCH_SIZE,\n","    teacher_kl = PPO_TEACHER_KL,\n","    kl_decay = PPO_KL_DECAY,\n","    max_grad_norm = PPO_MAX_GRAD_NORM,\n","    target_kl = PPO_TARGET_KL,\n","    val_interval = PPO_VAL_INTERVAL,\n","    index_ranges = INDEXES,\n","    debug = PPO_DEBUG\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ubXwir_PqMqm"},"outputs":[],"source":["# @title Fine tuning\n","\n","# Параметры\n","SL_MODEL_CONFIG = \"/content/drive/MyDrive/stocks/pretrained_rl_ppo/sl_weights/exp1/config.joblib\"\n","PPO_SAVE_PATH = '/content/drive/MyDrive/stocks/pretrained_rl_ppo/ppo_weights/ft'\n","CONFIG = joblib.load(SL_MODEL_CONFIG)\n","\n","SPLITS = (0.83, 0.12, 0.05)\n","BEST_PATH_ACTOR = \"/content/drive/MyDrive/stocks/pretrained_rl_ppo/ppo_weights/exp2/actor_best_1.weights.h5\"\n","BEST_CRITIC_PATH = \"/content/drive/MyDrive/stocks/pretrained_rl_ppo/ppo_weights/exp2/critic_best_1.weights.h5\"\n","BEST_BACKBONE_PATH = \"\"\n","FEATURES = CONFIG['features']\n","ACCOUNT_F = CONFIG['state']\n","\n","# @title PPO training parameters\n","PPO_SEQ_LEN = CONFIG['seq_len'] # ширина окна (такая же как при обучении SL)\n","PPO_FEATURES_DIM = len(FEATURES) + len(ACCOUNT_F)\n","PPO_NUM_ACTIONS = CONFIG['num_actions'] # кол-во возможных действий (такое же как при обучении SL)\n","PPO_UNITS_PER_LAYER = CONFIG['units_per_layer'] #  кол-во нейронов (такое же как при обучении SL)\n","PPO_DROPOUT = CONFIG['dropout']\n","\n","# INDEXES = [(\"2024-05-01\", 1000), (\"2024-07-01\", 500)]\n","\n","# PPO training parameters\n","PPO_UPDATES = 2\n","PPO_N_ENV = 10\n","PPO_ROLLOUT = 256\n","PPO_ACTOR_LR = 3e-3\n","PPO_CRITIC_LR = 1e-3\n","PPO_CLIP_RATIO = 1.5\n","PPO_C1 = 0.5\n","PPO_C2 = 0.02\n","PPO_EPOCHS = 10\n","PPO_BATCH_SIZE = 64\n","PPO_TEACHER_KL = 0.05\n","PPO_KL_DECAY = 0.5\n","PPO_TARGET_KL = 0.001\n","PPO_MAX_GRAD_NORM = 0.5\n","PPO_VAL_INTERVAL = 1\n","PPO_DEBUG = True\n","PPO_FUNE_TUNE = True\n","\n","TRAIN_CFG = EnvConfig(\n","    mode=1,\n","    fee=0.0002,\n","    spread=0.0001,\n","    leverage=1.0,\n","    max_steps=PPO_ROLLOUT+PPO_SEQ_LEN,\n","    reward_scale=1.0,\n","    use_log_reward=False,\n","    time_penalty=0.0,\n","    hold_penalty=0.0,\n",")\n","\n","VAL_CFG = EnvConfig(\n","    mode=1,\n","    fee=0.0002,\n","    spread=0.0001,\n","    leverage=1.0,\n","    max_steps=5000,\n","    reward_scale=1.0,\n","    use_log_reward=False,\n","    time_penalty=0.0,\n","    hold_penalty=0.0,\n",")\n","\n","actor, critic, train_log, val_log = train(\n","    ppo_train_df,\n","    ppo_val_df,\n","    TRAIN_CFG,\n","    VAL_CFG,\n","    PPO_FEATURES_DIM,\n","    FEATURES,\n","    seq_len = PPO_SEQ_LEN,\n","    teacher_weights = BEST_PATH_ACTOR,\n","    critic_weights = BEST_CRITIC_PATH,\n","    backbone_weights = BEST_BACKBONE_PATH,\n","    save_path = PPO_SAVE_PATH,\n","    num_actions = PPO_NUM_ACTIONS,\n","    units_per_layer = PPO_UNITS_PER_LAYER,\n","    dropout = PPO_DROPOUT,\n","    updates = PPO_UPDATES,\n","    n_env = PPO_N_ENV,\n","    rollout = PPO_ROLLOUT,\n","    actor_lr = PPO_ACTOR_LR,\n","    critic_lr = PPO_CRITIC_LR,\n","    clip_ratio = PPO_CLIP_RATIO,\n","    c1 = PPO_C1,\n","    c2 = PPO_C2,\n","    epochs = PPO_EPOCHS,\n","    batch_size = PPO_BATCH_SIZE,\n","    teacher_kl = PPO_TEACHER_KL,\n","    kl_decay = PPO_KL_DECAY,\n","    max_grad_norm = PPO_MAX_GRAD_NORM,\n","    target_kl = PPO_TARGET_KL,\n","    val_interval = PPO_VAL_INTERVAL,\n","    # index_ranges = INDEXES,\n","    debug = PPO_DEBUG,\n","    fine_tune = PPO_FUNE_TUNE\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0K5VozBzVMXh"},"outputs":[],"source":["# @title Test simulation\n","ACTOR_WEIGHTS = \"/content/drive/MyDrive/stocks/pretrained_rl_ppo/ppo_weights/exp2/actor_best_ep:12_rt:0.0743.weights.h5\"\n","SL_MODEL_CONFIG = \"/content/drive/MyDrive/stocks/pretrained_rl_ppo/sl_weights/exp3/config.joblib\"\n","CONFIG = joblib.load(SL_MODEL_CONFIG)\n","\n","START = \"2024-01-20\"\n","END = \"2025-09-10\"\n","TEST_FROM = \"2025-05-01\"\n","\n","# Загружаем параметры из сохраненного конфига\n","PPO_SEQ_LEN = CONFIG['seq_len']\n","PPO_FEATURES_DIM = len(CONFIG['features']) + len(CONFIG['state'])\n","PPO_UNITS_PER_LAYER = CONFIG['units_per_layer']\n","PPO_DROPOUT = CONFIG['dropout']\n","PPO_NUM_ACTIONS = CONFIG['num_actions']\n","FEATURES = CONFIG['features']\n","\n","dataframe = df.loc[START:END].copy()\n","\n","_,_,ppo_test_df, _ = prepare_datasets(dataframe,\n","                                  FEATURES,\n","                                  test_from=TEST_FROM,\n","                                  norm_kind='minmax')\n","\n","TEST_CFG = EnvConfig(\n","    mode=1,\n","    fee=0.0002,\n","    spread=0.0,\n","    leverage=1.0,\n","    max_steps=1e9,\n","    reward_scale=100.0,\n","    use_log_reward=False,\n","    valid_time=50,\n","    time_penalty=0.001,\n","    hold_penalty=0.001,\n",")\n","\n","testing_simulation(\n","    ppo_test_df,\n","    ACTOR_WEIGHTS,\n","    PPO_SEQ_LEN,\n","    PPO_FEATURES_DIM,\n","    PPO_UNITS_PER_LAYER,\n","    PPO_DROPOUT,\n","    PPO_NUM_ACTIONS,\n","    FEATURES,\n","    TEST_CFG,\n","    debug=False\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"mount_file_id":"1r8fak10hT1Bt10B7sUgSX6AxVu_a5M74","authorship_tag":"ABX9TyMTmEdYT/E62mVrbjWTqbE1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}