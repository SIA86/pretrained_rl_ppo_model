   "execution_count": null,
   "outputs": [],
   "execution_count": null,
   "outputs": [],
   "execution_count": null,
   "execution_count": null,
   "execution_count": null,
   "outputs": [],
   "execution_count": null,
   "outputs": [],
   "execution_count": null,
   "execution_count": null,
   "outputs": [],
   "outputs": [],
   "execution_count": null,
   "outputs": [],
   "execution_count": null,
   "outputs": [],
   "execution_count": null,
   "outputs": [],
   "execution_count": null,
   "outputs": [],
   "outputs": [],
   "outputs": [],
   "execution_count": null,
    "PPO_TEACHER_WEIGHTS = BEST_PATH\n",
    "PPO_BACKBONE_WEIGHTS = BEST_PATH\n",
   "execution_count": null,
   "outputs": [],
    "from scr.ppo_training import train\n",
    "from scr.backtest_env import EnvConfig\n",
    "actor, critic, train_log, val_log = train(\n",
    "    df=enriched,\n",
    "    teacher_weights=PPO_TEACHER_WEIGHTS,\n",
    "    backbone_weights=PPO_BACKBONE_WEIGHTS,\n",
    "    num_actions=PPO_NUM_ACTIONS,\n",
    "    units_per_layer=PPO_UNITS_PER_LAYER,\n",
    "    dropout=PPO_DROPOUT,\n",
    "    updates=PPO_UPDATES,\n",
    "    n_env=PPO_N_ENV,\n",
    "    rollout=PPO_ROLLOUT,\n",
    "    actor_lr=PPO_ACTOR_LR,\n",
    "    critic_lr=PPO_CRITIC_LR,\n",
    "    clip_ratio=PPO_CLIP_RATIO,\n",
    "    c1=PPO_C1,\n",
    "    c2=PPO_C2,\n",
    "    epochs=PPO_EPOCHS,\n",
    "    batch_size=PPO_BATCH_SIZE,\n",
    "    kl_decay=PPO_KL_DECAY,\n",
    "    max_grad_norm=PPO_MAX_GRAD_NORM,\n",
    "    target_kl=PPO_TARGET_KL,\n",
    "    val_interval=PPO_VAL_INTERVAL,\n",
    "    debug=PPO_DEBUG,\n",
    ")\n"
