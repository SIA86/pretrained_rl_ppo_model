    "PPO_TEACHER_WEIGHTS = BEST_PATH\n",
"PPO_BACKBONE_WEIGHTS = BEST_PATH\n",
"from scr.ppo_training import train\n",
"from scr.backtest_env import EnvConfig\n",
"cfg = EnvConfig(1, 0.0, 0.0, 1.0, len(enriched) - 1, 1.0, False, 0.0, 0.0)\n",
"actor, critic, train_log, val_log = train(\n",
"    df=enriched,\n",
"    cfg=cfg,\n",
"    seq_len=PPO_SEQ_LEN,\n",
"    teacher_weights=PPO_TEACHER_WEIGHTS,\n",
"    backbone_weights=PPO_BACKBONE_WEIGHTS,\n",
"    save_path=PPO_SAVE_PATH,\n",
"    num_actions=PPO_NUM_ACTIONS,\n",
"    units_per_layer=PPO_UNITS_PER_LAYER,\n",
"    dropout=PPO_DROPOUT,\n",
"    updates=PPO_UPDATES,\n",
"    n_env=PPO_N_ENV,\n",
"    rollout=PPO_ROLLOUT,\n",
"    actor_lr=PPO_ACTOR_LR,\n",
"    critic_lr=PPO_CRITIC_LR,\n",
"    clip_ratio=PPO_CLIP_RATIO,\n",
"    c1=PPO_C1,\n",
"    c2=PPO_C2,\n",
"    epochs=PPO_EPOCHS,\n",
"    batch_size=PPO_BATCH_SIZE,\n",
"    teacher_kl=PPO_TEACHER_KL,\n",
"    kl_decay=PPO_KL_DECAY,\n",
"    max_grad_norm=PPO_MAX_GRAD_NORM,\n",
"    target_kl=PPO_TARGET_KL,\n",
"    val_interval=PPO_VAL_INTERVAL,\n",
")\n",
